# -*- coding: utf-8 -*-
"""SQlines_scraper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dWZ7Nfta4ZL4Yiw9cVwePRP8dU9rJ1Ic
"""

import requests
from bs4 import BeautifulSoup
import json
import re

def scrape_sybase_to_oracle(url):
    """
    Scrapes a given URL to extract all text and table data, then saves it to a JSON file.

    This script is designed to parse the content of the SQLines Sybase to Oracle migration guide,
    extracting meaningful text blocks and all data conversion tables. The output is structured
    for potential use in fine-tuning a large language model.

    Args:
        url (str): The URL of the website to scrape.

    Returns:
        str: The name of the JSON file where the data is saved, or None if scraping fails.
    """
    try:
        # 1. Fetch the HTML content of the page
        print(f"Fetching content from {url}...")
        response = requests.get(url, timeout=10)
        # Raise an exception for bad status codes (4xx or 5xx)
        response.raise_for_status()
        print("Content fetched successfully.")

        # 2. Parse the HTML using BeautifulSoup
        soup = BeautifulSoup(response.content, 'html.parser')

        # 3. Extract the main title of the page
        page_title = soup.find('h1').get_text(strip=True) if soup.find('h1') else 'No Title Found'

        # 4. Extract all meaningful text content
        # We will target paragraphs, headings (h2, h3), and list items.
        print("Extracting text content...")
        texts = []
        # Find the main content div to avoid scraping headers, footers, and sidebars.
        # Based on inspection, the main content is within a 'div' with id 'content'.
        content_div = soup.find('div', id='content')
        if content_div:
            # Find all relevant tags within the content div
            for tag in content_div.find_all(['h2', 'h3', 'p', 'li']):
                # Clean up the text: remove extra whitespace and newlines
                text = re.sub(r'\s+', ' ', tag.get_text(strip=True))
                if text: # Only add non-empty text
                    texts.append({'type': tag.name, 'content': text})
        print(f"Extracted {len(texts)} text blocks.")

        # 5. Extract all tables
        print("Extracting tables...")
        tables = []
        # Find all table elements on the page
        for table_tag in soup.find_all('table'):
            table_data = []
            # Extract header row (thead -> th)
            header_row = []
            for th in table_tag.find_all('th'):
                header_row.append(th.get_text(strip=True))

            if header_row:
                table_data.append(header_row)

            # Extract body rows (tbody -> tr -> td)
            for row_tag in table_tag.find('tbody').find_all('tr') if table_tag.find('tbody') else table_tag.find_all('tr'):
                row_data = []
                for cell in row_tag.find_all('td'):
                    # Clean up cell text
                    cell_text = re.sub(r'\s+', ' ', cell.get_text(strip=True))
                    row_data.append(cell_text)
                if row_data:
                    table_data.append(row_data)

            if table_data:
                tables.append(table_data)
        print(f"Extracted {len(tables)} tables.")

        # 6. Structure the data into a dictionary
        scraped_data = {
            'source_url': url,
            'page_title': page_title,
            'extracted_texts': texts,
            'extracted_tables': tables
        }

        # 7. Save the data to a JSON file
        output_filename = 'sybase_to_oracle_data.json'
        print(f"Saving data to {output_filename}...")
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(scraped_data, f, ensure_ascii=False, indent=4)

        print("Scraping and data extraction complete.")
        return output_filename

    except requests.exceptions.RequestException as e:
        print(f"Error: Could not fetch the URL. {e}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None

if __name__ == '__main__':
    # The target URL for scraping
    TARGET_URL = 'https://www.sqlines.com/sybase-to-oracle'

    # Run the scraper function
    scrape_sybase_to_oracle(TARGET_URL)

